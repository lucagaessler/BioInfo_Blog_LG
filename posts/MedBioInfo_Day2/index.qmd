---
title: "Applied Bioinformatics Day 2"
author: "Luca Gaessler"
date: "2025-10-07"
categories: [course, medbioinfo, learning]
description: "Hosted by Sweden's School for Medical Bioinformatics"
---

# Key Concepts I Learned Today

- **Coding Environments**
- **Containerization**

### Coding Environments
Today's session was all about reproducibility. How can we as computational biologists ensure that our tools always give the same results? This question becomes especially relevant when sharing code with collaborators, which might use different operating systems or have packages in different versions installed. 

Coding environments like Pixi can help us not only to keep track of the tools and the versions that we use in our analyses but also to enforce them within the limits of a project. A Pixi-controlled environment covers one directory as well as its subdirectories, and is created similarly to a GitHub repository:

```bash
pixi init [channel]
```

By providing channels, i.e. the tool space that Pixi can choose from, we define where our packages are actually coming from. For example, we provided the Conda-Forge and Bioconda databases. Packages can then be easily installed and run using the following commands:

```bash
pixi add [package]

pixi run [command]
```

Alternatively, the Pixi environment can also be accessed as its own shell. 

### Containerization
Containers work quite similarly to environments but isolate its content even more from the machine they are running on. Even the operating system can be different. This allows the users to apply different versions of one tool on their computer and ensures a high degree of reproducibility when sharing scripts and the corresponding container. Containers are stored and shared as "container images", a type of metadata. These images can be conveniently downloaded from Websites like "docker.com" and then be used to build the actual container. Prominent examples container managment softwares are Docker, often used on individual computers, and Apptainer, often the tool of choice for Linux-based computer clusters.

Containers can be run both from the inside (more for exploratory analyses) and the outside (once the workflow is established):

```bash
apptainer shell [container] #inside

apptainer exec [container] [command] #outside
```

It is possible to build your own container, giving you all the power over its operating system, dependencies, etc. Still, this task is often computationally extensive and in most cases, a suitable container with the desired tool is already available to download from the web.

# Reflections
Naturally, there is no perfect solution among the two approaches and both have their Pros & Cons. While environments like Pixi let you interact with outside directories more easily than containers, tools like Docker provide you with a higher degree of control over the conditions in which your analyses take place. In the end, it is all about finding a tool that balances outcome and effort according to the situation. The important part is to increase the reproducibility of your science, and both tools are a valuable addition for that.

# Next Steps
Tomorrow will be all about bioinformatic pipelines and how to built them! Stay tuned for my next post, which will be about...

Netxflow!

**Thanks for reading!**