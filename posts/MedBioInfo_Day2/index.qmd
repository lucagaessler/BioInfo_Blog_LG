---
title: "Applied Bioinformatics Day 2"
author: "Luca Gaessler"
date: "2025-10-07"
image: "docker-mark-blue.png"
categories: [course, medbioinfo, Docker]
description: "Hosted by Sweden's School for Medical Bioinformatics"
---

## Key Concepts I Learned Today

- **Coding Environments**
- **Containerization**

### Coding Environments
Today's session was all about reproducibility. How can we, as computational biologists, ensure that our tools always give the same results? This question becomes especially relevant when we share code with collaborators, which might use different operating systems or have packages installed in different versions. 

Coding environments like Pixi can help us not only to keep track of the tools and the versions that we use in our analyses but also to enforce them within the limits of a project. A Pixi-controlled environment covers one directory as well as its subdirectories, and is created similarly to a GitHub repository:

```bash
pixi init [channel]
```

By providing channels, i.e. the tool space that Pixi can choose from, we define where our packages are coming from. For example, we provided the Conda-Forge and Bioconda databases. Packages can then be easily installed and run using the following commands:

```bash
pixi add [package]

pixi run [command]
```

Alternatively, the Pixi environment can also be set up and accessed as its own shell. 

### Containerization
Containers work quite similarly to environments like Pixi but isolate their content even more from the machine they are running on. Even the operating system can be different within the container. This allows the users to apply different versions of the same tool on one computer. Additionally, containers ensure a high degree of reproducibility when shared. They are stored and transferred as "container images", a specific type of metadata. These images can be conveniently downloaded from Websites like "docker.com" and then be used to build the actual container. Prominent examples of container managment softwares are Docker, often used on individual computers, and Apptainer, which is often the tool of choice for Linux-based computer clusters.

Containers can be run both from inside (suitable for exploratory analyses) and outside (once the workflow is established):

```bash
apptainer shell [container] #inside

apptainer exec [container] [command] #outside
```

It is possible to build your own container, giving you all the power over its operating system, dependencies, versions, etc. On the other hand, this task is often computationally extensive and in most cases, a suitable container with the desired tool(s) is already available for download from the web.

## Reflections
Naturally, there is no perfect solution among the two approaches and both have their Pros & Cons. While environments like Pixi let you interact with outside directories more easily than containers, tools like Docker provide you with a higher degree of control over the conditions in which your analyses take place. In the end, it is all about finding a tool that balances outcome and effort according to the situation. The important part is to increase the reproducibility of your science, and both tools are a valuable addition to that.

## Next Steps
Tomorrow will be all about bioinformatic pipelines and how to built them! Stay tuned for my next post, which will be about...

Nextflow!

**Thanks for reading!**