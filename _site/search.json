[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html",
    "href": "posts/MedBioInfo_Day1/index.html",
    "title": "Applied Bioinformatics Day 1",
    "section": "",
    "text": "Key Concepts I Learned Today\n\nCollaborating in GitHub\nQuarto Blogs\nGitHub Pages & Actions\n\n\nCollaborating in GitHub\nToday’s practical session was initiated by a group task. The goal was to create an ordered list of all participant’s names. An easy task at first but not if you consider the following rule: “Each student can only write their own name in a stand-alone file.”. This means that the course members had to collaborate on GitHub to merge their documents and handle potential conflicts. The idea was to work in pairs, then collaborate with the merged list of another pair, and so on… The final workflow turned out to be less structured but we still managed to come up with the full list in the end.\nMerging the document looked as follows:\ngit pull # update local repository\ngit checkout [branch_1] # switch to branch that will be merged into\ngit merge [branch_2] # merge the two branches\n# solve conflicts\ngit add # stage changes\ngit commit # commit changes\ngit push # add changes to remote repository\n\n\nQuarto Blogs\nQuarto has been developed for literate programming, meaning that it can combine code with markdown text. Quarto can be used to generate beautiful output documents, including the HTTP format for websites and blogs. To test its capabilities, each course member will follow along this course, “Applied Bioinformatics”, in a blog format - one post for each day. Of course version-controlled via Git(Hub)!\n\n\nGitHub Pages & Actions\nEach member set up his blog as a public website via GitHub Pages. Now whenever local changes are pushed to the remote repository, GitHub will re-build the website, thereby updating its contents. This functionality is provided by GitHub Actions.\n\n\n\nReflections\nIn conclusion, the first day was a great start into the course! Now that we have laid the foundation for collaborative work via GitHub as well as our primary medium of documentation, the Quarto Blog, we are ready to dive into the complex topics of Bioinformatics!\n\n\nNext Steps\nNext, I am particularly interested in concepts regarding containerization as well as pipeline building. In the context of this course, tools for both (Docker, Nextflow) will be highlighted in the upcoming days!\nThanks for reading!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Bioinformatics Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioinfo Blog",
    "section": "",
    "text": "Applied Bioinformatics Day 2\n\n\n\ncourse\n\nmedbioinfo\n\nlearning\n\n\n\nHosted by Sweden’s School for Medical Bioinformatics\n\n\n\nLuca Gaessler\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Bioinformatics Blog\n\n\n\nnews\n\nbioinfo\n\n\n\n\n\n\n\nLuca Gaessler\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nApplied Bioinformatics Day 1\n\n\n\ncourse\n\nmedbioinfo\n\nlearning\n\n\n\nHosted by Sweden’s School for Medical Bioinformatics\n\n\n\nLuca Gaessler\n\n\nOct 6, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html#collaborating-in-github",
    "href": "posts/MedBioInfo_Day1/index.html#collaborating-in-github",
    "title": "Applied Bioinformatics Day 1",
    "section": "",
    "text": "Today’s practical session was initiated by a group task. The goal was to create an ordered list of all participant’s names. An easy task at first but not if you consider the following rule: “Each student can only write their own name in a stand-alone file.”. This means that the course members had to collaborate on GitHub to merge their documents and handle potential conflicts. The idea was to work in pairs, then collaborate with the merged list of another pair, and so on… The final workflow turned out to be less structured but we still managed to come up with the full list in the end.\nMerging the document looked as follows:\ngit pull # update local repository\ngit checkout [branch_1] # switch to branch that will be merged into\ngit merge [branch_2] # merge the two branches\n# solve conflicts\ngit add # stage changes\ngit commit # commit changes\ngit push # add changes to remote repository"
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html#quarto-blogs",
    "href": "posts/MedBioInfo_Day1/index.html#quarto-blogs",
    "title": "Applied Bioinformatics Day 1",
    "section": "",
    "text": "Quarto has been developed for literate programming, meaning that it can combine code with markdown text. Quarto can be used to generate beautiful output documents, including the HTTP format for websites and blogs. To test its capabilities, each course member will follow along this course, “Applied Bioinformatics”, in a blog format - one post for each day. Of course version-controlled via Git(Hub)!"
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html#github-actions",
    "href": "posts/MedBioInfo_Day1/index.html#github-actions",
    "title": "Applied Bioinformatics Day 1",
    "section": "",
    "text": "Each member set up his blog as a public website via GitHub Pages. Now whenever local changes are pushed to the remote repository, GitHub will re-build the website, thereby updating its contents. This functionality is provided by GitHub Actions."
  },
  {
    "objectID": "posts/MedBioInfo_Day2/index.html",
    "href": "posts/MedBioInfo_Day2/index.html",
    "title": "Applied Bioinformatics Day 2",
    "section": "",
    "text": "Key Concepts I Learned Today\n\nCoding Environments\nContainerization\n\n\nCoding Environments\nToday’s session was all about reproducibility. How can we as computational biologists ensure that our tools always give the same results? This question becomes especially relevant when sharing code with collaborators, which might use different operating systems or have packages in different versions installed.\nCoding environments like Pixi can help us not only to keep track of the tools and the versions that we use in our analyses but also to enforce them within the limits of a project. A Pixi-controlled environment covers one directory as well as its subdirectories, and is created similarly to a GitHub repository:\npixi init [channel]\nBy providing channels, i.e. the tool space that Pixi can choose from, we define where our packages are actually coming from. For example, we provided the Conda-Forge and Bioconda databases. Packages can then be easily installed and run using the following commands:\npixi add [package]\n\npixi run [command]\nAlternatively, the Pixi environment can also be accessed as its own shell.\n\n\nContainerization\nContainers work quite similarly to environments but isolate its content even more from the machine they are running on. Even the operating system can be different. This allows the users to apply different versions of one tool on their computer and ensures a high degree of reproducibility when sharing scripts and the corresponding container. Containers are stored and shared as “container images”, a type of metadata. These images can be conveniently downloaded from Websites like “docker.com” and then be used to build the actual container. Prominent examples container managment softwares are Docker, often used on individual computers, and Apptainer, often the tool of choice for Linux-based computer clusters.\nContainers can be run both from the inside (more for exploratory analyses) and the outside (once the workflow is established):\napptainer shell [container] #inside\n\napptainer exec [container] [command] #outside\nIt is possible to build your own container, giving you all the power over its operating system, dependencies, etc. Still, this task is often computationally extensive and in most cases, a suitable container with the desired tool is already available to download from the web.\n\n\n\nReflections\nNaturally, there is no perfect solution among the two approaches and both have their Pros & Cons. While environments like Pixi let you interact with outside directories more easily than containers, tools like Docker provide you with a higher degree of control over the conditions in which your analyses take place. In the end, it is all about finding a tool that balances outcome and effort according to the situation. The important part is to increase the reproducibility of your science, and both tools are a valuable addition for that.\n\n\nNext Steps\nTomorrow will be all about bioinformatic pipelines and how to built them! Stay tuned for my next post, which will be about…\nNetxflow!\nThanks for reading!"
  }
]