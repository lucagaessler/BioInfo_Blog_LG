[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Bioinformatics Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/MedBioInfo_Day3/index.html",
    "href": "posts/MedBioInfo_Day3/index.html",
    "title": "Applied Bioinformatics Day 3",
    "section": "",
    "text": "Nextflow\nnf-core\n\n\n\nNextflow is a tool to connect different parts of your analysis. While this might trigger reactions like “I can run my scripts myself” and “Nice to have”, the use of workflow managers is real and multifold. Nextflow scripts not only lower manual efforts, and thereby the possibility of human erros, but also enhance analytical scalability and reproducibility. When scripts are run over many samples, high-performance clusters are essential. When transferring scripts from local computers to clusters, Nexflow is a game-changer. It always keeps the functional logic of a workflow separate from its execution, meaning that you only have to exchange your local for appropriate server parameters to scale up your analysis. Additionally, Nextflow inherently incorporates version control and containerization, which - as I told about in the previous posts - are real boosters of science reproducibility.\nA very useful function in Nextflow is the “resume” option. Imagine your pipeline has a bug right after a computationally expensive calculation. After fixing the bug, “resume” allows you to re-run the now working pipeline but all unchanged steps are skipped and their results recovered from cache instead.\n\n\n\n\n\n\nFigure 1: Basic Nextflow Archictecture. Processes, which contain the executable scripts, are connected via channels that move the data along the workflow. Processes are modular and can therefore be written in different programming languages.\n\n\n\n\n\n\nBuilding up on Nextflow, nf-core is a web-platform that provides curated & ready-to-use bioinformatic pipelines. The platform is largely community-driven. Volunteers create and validate the provided pipelines (at the time of this post 139x) - the code is freely available on GitHub. Judging form my experience, most of the available pipelines are built for data processing tasks, while the focus lies less on downstream biological analyses. A very prominent use-case of nf-core is their rnaseq pipeline to generate count matrices form raw FASTQ files, including quality control vias FastQC."
  },
  {
    "objectID": "posts/MedBioInfo_Day3/index.html#key-concepts-i-learned-today",
    "href": "posts/MedBioInfo_Day3/index.html#key-concepts-i-learned-today",
    "title": "Applied Bioinformatics Day 3",
    "section": "",
    "text": "Nextflow\nnf-core\n\n\n\nNextflow is a tool to connect different parts of your analysis. While this might trigger reactions like “I can run my scripts myself” and “Nice to have”, the use of workflow managers is real and multifold. Nextflow scripts not only lower manual efforts, and thereby the possibility of human erros, but also enhance analytical scalability and reproducibility. When scripts are run over many samples, high-performance clusters are essential. When transferring scripts from local computers to clusters, Nexflow is a game-changer. It always keeps the functional logic of a workflow separate from its execution, meaning that you only have to exchange your local for appropriate server parameters to scale up your analysis. Additionally, Nextflow inherently incorporates version control and containerization, which - as I told about in the previous posts - are real boosters of science reproducibility.\nA very useful function in Nextflow is the “resume” option. Imagine your pipeline has a bug right after a computationally expensive calculation. After fixing the bug, “resume” allows you to re-run the now working pipeline but all unchanged steps are skipped and their results recovered from cache instead.\n\n\n\n\n\n\nFigure 1: Basic Nextflow Archictecture. Processes, which contain the executable scripts, are connected via channels that move the data along the workflow. Processes are modular and can therefore be written in different programming languages.\n\n\n\n\n\n\nBuilding up on Nextflow, nf-core is a web-platform that provides curated & ready-to-use bioinformatic pipelines. The platform is largely community-driven. Volunteers create and validate the provided pipelines (at the time of this post 139x) - the code is freely available on GitHub. Judging form my experience, most of the available pipelines are built for data processing tasks, while the focus lies less on downstream biological analyses. A very prominent use-case of nf-core is their rnaseq pipeline to generate count matrices form raw FASTQ files, including quality control vias FastQC."
  },
  {
    "objectID": "posts/MedBioInfo_Day3/index.html#reflections",
    "href": "posts/MedBioInfo_Day3/index.html#reflections",
    "title": "Applied Bioinformatics Day 3",
    "section": "Reflections",
    "text": "Reflections\nAlthough I currently see myself more as data scientist with a focus on biological analyses, workflow managers and off-the-shelf pipelines will definitely play an important part in the future of my PhD projects. Biological datasets are getting larger and larger, which calls for increased scalability and the need for high-performance computing clusters. All of this is facilitated by tools like Nextflow and will definitely change the way I work right now."
  },
  {
    "objectID": "posts/MedBioInfo_Day3/index.html#next-steps",
    "href": "posts/MedBioInfo_Day3/index.html#next-steps",
    "title": "Applied Bioinformatics Day 3",
    "section": "Next Steps",
    "text": "Next Steps\nWhile we have only scratched the surface of bioinformatic pipelines today, tomorrow will bring more hands-on tasks on Nextflow as well as the use of nf-core pipelines.\nThanks for reading!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioinfo Blog",
    "section": "",
    "text": "Applied Bioinformatics Day 3\n\n\n\ncourse\n\nmedbioinfo\n\nlearning\n\n\n\nHosted by Sweden’s School for Medical Bioinformatics\n\n\n\nLuca Gaessler\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nApplied Bioinformatics Day 2\n\n\n\ncourse\n\nmedbioinfo\n\nlearning\n\n\n\nHosted by Sweden’s School for Medical Bioinformatics\n\n\n\nLuca Gaessler\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Bioinformatics Blog\n\n\n\nnews\n\nbioinfo\n\n\n\n\n\n\n\nLuca Gaessler\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nApplied Bioinformatics Day 1\n\n\n\ncourse\n\nmedbioinfo\n\nlearning\n\n\n\nHosted by Sweden’s School for Medical Bioinformatics\n\n\n\nLuca Gaessler\n\n\nOct 6, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MedBioInfo_Day2/index.html",
    "href": "posts/MedBioInfo_Day2/index.html",
    "title": "Applied Bioinformatics Day 2",
    "section": "",
    "text": "Coding Environments\nContainerization\n\n\n\nToday’s session was all about reproducibility. How can we as computational biologists ensure that our tools always give the same results? This question becomes especially relevant when sharing code with collaborators, which might use different operating systems or have packages in different versions installed.\nCoding environments like Pixi can help us not only to keep track of the tools and the versions that we use in our analyses but also to enforce them within the limits of a project. A Pixi-controlled environment covers one directory as well as its subdirectories, and is created similarly to a GitHub repository:\npixi init [channel]\nBy providing channels, i.e. the tool space that Pixi can choose from, we define where our packages are actually coming from. For example, we provided the Conda-Forge and Bioconda databases. Packages can then be easily installed and run using the following commands:\npixi add [package]\n\npixi run [command]\nAlternatively, the Pixi environment can also be accessed as its own shell.\n\n\n\nContainers work quite similarly to environments but isolate its content even more from the machine they are running on. Even the operating system can be different. This allows the users to apply different versions of one tool on their computer and ensures a high degree of reproducibility when sharing scripts and the corresponding container. Containers are stored and shared as “container images”, a type of metadata. These images can be conveniently downloaded from Websites like “docker.com” and then be used to build the actual container. Prominent examples container managment softwares are Docker, often used on individual computers, and Apptainer, often the tool of choice for Linux-based computer clusters.\nContainers can be run both from the inside (more for exploratory analyses) and the outside (once the workflow is established):\napptainer shell [container] #inside\n\napptainer exec [container] [command] #outside\nIt is possible to build your own container, giving you all the power over its operating system, dependencies, etc. Still, this task is often computationally extensive and in most cases, a suitable container with the desired tool is already available to download from the web."
  },
  {
    "objectID": "posts/MedBioInfo_Day2/index.html#key-concepts-i-learned-today",
    "href": "posts/MedBioInfo_Day2/index.html#key-concepts-i-learned-today",
    "title": "Applied Bioinformatics Day 2",
    "section": "",
    "text": "Coding Environments\nContainerization\n\n\n\nToday’s session was all about reproducibility. How can we as computational biologists ensure that our tools always give the same results? This question becomes especially relevant when sharing code with collaborators, which might use different operating systems or have packages in different versions installed.\nCoding environments like Pixi can help us not only to keep track of the tools and the versions that we use in our analyses but also to enforce them within the limits of a project. A Pixi-controlled environment covers one directory as well as its subdirectories, and is created similarly to a GitHub repository:\npixi init [channel]\nBy providing channels, i.e. the tool space that Pixi can choose from, we define where our packages are actually coming from. For example, we provided the Conda-Forge and Bioconda databases. Packages can then be easily installed and run using the following commands:\npixi add [package]\n\npixi run [command]\nAlternatively, the Pixi environment can also be accessed as its own shell.\n\n\n\nContainers work quite similarly to environments but isolate its content even more from the machine they are running on. Even the operating system can be different. This allows the users to apply different versions of one tool on their computer and ensures a high degree of reproducibility when sharing scripts and the corresponding container. Containers are stored and shared as “container images”, a type of metadata. These images can be conveniently downloaded from Websites like “docker.com” and then be used to build the actual container. Prominent examples container managment softwares are Docker, often used on individual computers, and Apptainer, often the tool of choice for Linux-based computer clusters.\nContainers can be run both from the inside (more for exploratory analyses) and the outside (once the workflow is established):\napptainer shell [container] #inside\n\napptainer exec [container] [command] #outside\nIt is possible to build your own container, giving you all the power over its operating system, dependencies, etc. Still, this task is often computationally extensive and in most cases, a suitable container with the desired tool is already available to download from the web."
  },
  {
    "objectID": "posts/MedBioInfo_Day2/index.html#reflections",
    "href": "posts/MedBioInfo_Day2/index.html#reflections",
    "title": "Applied Bioinformatics Day 2",
    "section": "Reflections",
    "text": "Reflections\nNaturally, there is no perfect solution among the two approaches and both have their Pros & Cons. While environments like Pixi let you interact with outside directories more easily than containers, tools like Docker provide you with a higher degree of control over the conditions in which your analyses take place. In the end, it is all about finding a tool that balances outcome and effort according to the situation. The important part is to increase the reproducibility of your science, and both tools are a valuable addition for that."
  },
  {
    "objectID": "posts/MedBioInfo_Day2/index.html#next-steps",
    "href": "posts/MedBioInfo_Day2/index.html#next-steps",
    "title": "Applied Bioinformatics Day 2",
    "section": "Next Steps",
    "text": "Next Steps\nTomorrow will be all about bioinformatic pipelines and how to built them! Stay tuned for my next post, which will be about…\nNetxflow!\nThanks for reading!"
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html",
    "href": "posts/MedBioInfo_Day1/index.html",
    "title": "Applied Bioinformatics Day 1",
    "section": "",
    "text": "Collaborating in GitHub\nQuarto Blogs\nGitHub Pages & Actions\n\n\n\nToday’s practical session was initiated by a group task. The goal was to create an ordered list of all participant’s names. An easy task at first but not if you consider the following rule: “Each student can only write their own name in a stand-alone file.”. This means that the course members had to collaborate on GitHub to merge their documents and handle potential conflicts. The idea was to work in pairs, then collaborate with the merged list of another pair, and so on… The final workflow turned out to be less structured but we still managed to come up with the full list in the end.\nMerging the document looked as follows:\ngit pull # update local repository\ngit checkout [branch_1] # switch to branch that will be merged into\ngit merge [branch_2] # merge the two branches\n# solve conflicts\ngit add # stage changes\ngit commit # commit changes\ngit push # add changes to remote repository\n\n\n\nQuarto has been developed for literate programming, meaning that it can combine code with markdown text. Quarto can be used to generate beautiful output documents, including the HTTP format for websites and blogs. To test its capabilities, each course member will follow along this course, “Applied Bioinformatics”, in a blog format - one post for each day. Of course version-controlled via Git(Hub)!\n\n\n\nEach member set up his blog as a public website via GitHub Pages. Now whenever local changes are pushed to the remote repository, GitHub will re-build the website, thereby updating its contents. This functionality is provided by GitHub Actions."
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html#key-concepts-i-learned-today",
    "href": "posts/MedBioInfo_Day1/index.html#key-concepts-i-learned-today",
    "title": "Applied Bioinformatics Day 1",
    "section": "",
    "text": "Collaborating in GitHub\nQuarto Blogs\nGitHub Pages & Actions\n\n\n\nToday’s practical session was initiated by a group task. The goal was to create an ordered list of all participant’s names. An easy task at first but not if you consider the following rule: “Each student can only write their own name in a stand-alone file.”. This means that the course members had to collaborate on GitHub to merge their documents and handle potential conflicts. The idea was to work in pairs, then collaborate with the merged list of another pair, and so on… The final workflow turned out to be less structured but we still managed to come up with the full list in the end.\nMerging the document looked as follows:\ngit pull # update local repository\ngit checkout [branch_1] # switch to branch that will be merged into\ngit merge [branch_2] # merge the two branches\n# solve conflicts\ngit add # stage changes\ngit commit # commit changes\ngit push # add changes to remote repository\n\n\n\nQuarto has been developed for literate programming, meaning that it can combine code with markdown text. Quarto can be used to generate beautiful output documents, including the HTTP format for websites and blogs. To test its capabilities, each course member will follow along this course, “Applied Bioinformatics”, in a blog format - one post for each day. Of course version-controlled via Git(Hub)!\n\n\n\nEach member set up his blog as a public website via GitHub Pages. Now whenever local changes are pushed to the remote repository, GitHub will re-build the website, thereby updating its contents. This functionality is provided by GitHub Actions."
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html#reflections",
    "href": "posts/MedBioInfo_Day1/index.html#reflections",
    "title": "Applied Bioinformatics Day 1",
    "section": "Reflections",
    "text": "Reflections\nIn conclusion, the first day was a great start into the course! Now that we have laid the foundation for collaborative work via GitHub as well as our primary medium of documentation, the Quarto Blog, we are ready to dive into the complex topics of Bioinformatics!"
  },
  {
    "objectID": "posts/MedBioInfo_Day1/index.html#next-steps",
    "href": "posts/MedBioInfo_Day1/index.html#next-steps",
    "title": "Applied Bioinformatics Day 1",
    "section": "Next Steps",
    "text": "Next Steps\nNext, I am particularly interested in concepts regarding containerization as well as pipeline building. In the context of this course, tools for both (Docker, Nextflow) will be highlighted in the upcoming days!\nThanks for reading!"
  }
]